{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модуль предобработки данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn==1.1\n",
    "#%pip install --upgrade scikit-learn==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Имитируем подачу данных с датчиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "General_path = \"C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/\"\n",
    "raw = \"raw/\"\n",
    "processed = \"processed/\"\n",
    "final = \"final/\"\n",
    "tests = \"tests/\"\n",
    "\n",
    "# train_FD001 = pd.read_csv(General_path + \"train_FD001.csv\")\n",
    "# train_FD002 = pd.read_csv(General_path + \"train_FD002.csv\")\n",
    "# train_FD003 = pd.read_csv(General_path + \"train_FD003.csv\")\n",
    "# train_FD004 = pd.read_csv(General_path + \"train_FD004.csv\")\n",
    "\n",
    "# test_FD001 = pd.read_csv(General_path + \"test_FD001.csv\")\n",
    "# test_FD002 = pd.read_csv(General_path + \"test_FD002.csv\")\n",
    "# test_FD003 = pd.read_csv(General_path + \"test_FD003.csv\")\n",
    "# test_FD004 = pd.read_csv(General_path + \"test_FD004.csv\")\n",
    "\n",
    "# This dataset using for show preprocess handling incoming data\n",
    "# BAD_test_FD001 = pd.read_csv(General_path + \"BAD_test_FD001.csv\")\n",
    "\n",
    "# Small_BAD_test_FD001 = pd.read_csv(General_path + raw + \"Small_BAD_test_FD001.csv\")\n",
    "np_Small_BAD_test_FD001 = genfromtxt(General_path + raw + \"Small_BAD_test_FD001.csv\", delimiter=',')\n",
    "np_Small_GOOD_test_FD001 = genfromtxt(General_path + raw + \"Small_GOOD_test_FD001.csv\", delimiter=',')\n",
    "# Small_GOOD_test_FD001 = pd.read_csv(General_path + \"Small_GOOD_test_FD001.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Проверим \"плохой\" датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_Small_BAD_test_FD001:\n",
      "[[     nan      nan      nan      nan]\n",
      " [ 1.0e+00  1.0e+00      nan  3.0e-04]\n",
      " [ 1.0e+00  2.0e+00 -2.7e-03 -3.0e-04]\n",
      " [ 1.0e+00  3.0e+00  3.0e-04      nan]\n",
      " [ 1.0e+00      nan  4.2e-03  0.0e+00]]\n",
      "\n",
      "np_Small_GOOD_test_FD001:\n",
      "[[     nan      nan      nan      nan]\n",
      " [ 1.0e+00  1.0e+00  2.3e-03  3.0e-04]\n",
      " [ 1.0e+00  2.0e+00 -2.7e-03 -3.0e-04]\n",
      " [ 1.0e+00  3.0e+00  3.0e-04  1.0e-04]\n",
      " [ 1.0e+00  4.0e+00  4.2e-03  0.0e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"np_Small_BAD_test_FD001:\")\n",
    "print(np_Small_BAD_test_FD001)\n",
    "print(\"\\nnp_Small_GOOD_test_FD001:\")\n",
    "print(np_Small_GOOD_test_FD001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поиск пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_null(dataframe: pd.DataFrame):\n",
    "#     count_nulls = 0\n",
    "#     null_set = pd.Dadataframe(dataframe.isnull())\n",
    "\n",
    "#     for col in dataframe.columns:\n",
    "#         for val in dataframe[col].is_null():\n",
    "#             if (val == True):\n",
    "#                 count_nulls +=1\n",
    "#             #print(val)\n",
    "#         print(\"in \", col, \"nulls: \", count_nulls)\n",
    "#         count_nulls = 0\n",
    "\n",
    "# is_null(np_Small_BAD_test_FD001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем SimpleInputer - простой заполнитель пропущенных значений в датасете\n",
    "\n",
    "Так как предполагается, что данные упорядоченны по циклам работы двигателя, не целесообразно использовать стандартный заполнитель SimpleImputer. разумнее будет вычислить среднее значение лишь до и после пропущенного значения в каждом столбце. Можно использовать KNNImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_inputer = KNNImputer(n_neighbors = 2)\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "pipe_num = Pipeline([('imputer', simple_inputer),('scaler', std_scaler)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        , -1.5       ,  0.55797218,  1.42009389],\n",
       "       [ 0.        , -0.5       , -1.63015401, -1.67829278],\n",
       "       [ 0.        ,  0.5       , -0.3172783 ,  0.38729833],\n",
       "       [ 0.        ,  1.5       ,  1.38946013, -0.12909944]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_num = pipe_num.fit_transform(np_Small_GOOD_test_FD001)\n",
    "res_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можем применить полученный pipeline\n",
    "Видим, что пропусков нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_num = pd.DataFrame(res_num, columns=pipe_num['scaler'].get_feature_names_out(np_Small_BAD_test_FD001[0,:]))\n",
    "# res_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = 'C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/final/Pipelinr.pickle'\n",
    "\n",
    "with open(savePath, 'wb') as handle:\n",
    "    save_pik_pipeline = pickle.dumps(pipe_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_1.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_dir = 'C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/processed'\n",
    "save_dir = f\"{inp_dir}/\"\n",
    "files = os.listdir(inp_dir)\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    new_name = f'{save_dir}new_{file}'\n",
    "    os.rename(save_dir + file, new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dir = 'C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/raw/tests'\n",
    "out_dir = 'C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/processed'\n",
    "\n",
    "employ_Pipline(inp_dir,\n",
    "               out_dir,\n",
    "               pipe_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавляем столбец с индексами и удаляес строку с названием столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_count, col_count = np_Small_BAD_test_FD001.shape\n",
    "indexses = []\n",
    "indexses.append(0)\n",
    "print(\"str -->\", str_count,\"\\ncol -->\", col_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.0e+00,  1.0e+00,  1.0e+00,      nan,  3.0e-04],\n",
       "       [ 1.0e+00,  1.0e+00,  2.0e+00, -2.7e-03, -3.0e-04],\n",
       "       [ 2.0e+00,  1.0e+00,  3.0e+00,  3.0e-04,      nan],\n",
       "       [ 3.0e+00,  1.0e+00,      nan,  4.2e-03,  0.0e+00]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(str_count-1):\n",
    "    indexses.append(i)\n",
    "print(indexses)\n",
    "\n",
    "np_Small_BAD_test_FD001 = np.insert(np_Small_BAD_test_FD001, 0, indexses, axis= 1)\n",
    "np_Small_BAD_test_FD001 = np_Small_BAD_test_FD001[1:, 0:]\n",
    "np_Small_BAD_test_FD001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np_Small_BAD_test_FD001[1:,1:].astype(float)\n",
    "colums = np_Small_BAD_test_FD001[0, 1:]\n",
    "\n",
    "pd_Small_BAD_test_FD001 = pd.DataFrame(values, columns = colums)\n",
    "pd_Small_BAD_test_FD001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_type_pd(dataset : pd.DataFrame):\n",
    "    \n",
    "    check_type1: type = type(0.0)\n",
    "    check_type2: type = type(0)\n",
    "\n",
    "    colums = dataset.columns.tolist()\n",
    "\n",
    "    for col in colums:\n",
    "        for var in dataset[col]:\n",
    "            # if check_type1 == type(var) or check_type2 == type(var):\n",
    "            #     print(\"yes\")\n",
    "            # else:\n",
    "            #     print(\"no\")\n",
    "            print(f\"{var} -> {type(var)}\")\n",
    "\n",
    "check_type_pd(pd_Small_BAD_test_FD001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "def check_type_np(dataset: np.array):\n",
    "\n",
    "    dataset_v = np.delete(dataset, (0), axis=0)\n",
    "    res: bool\n",
    "    res = False\n",
    "\n",
    "    for st in dataset_v:\n",
    "        for col in st:\n",
    "            # print(f\"{col} --> {type(col)}\")\n",
    "            if(np.isnan(col)):\n",
    "                res = True\n",
    "                return res\n",
    "    \n",
    "    return res\n",
    "            \n",
    "            \n",
    "print(check_type_np(np_Small_GOOD_test_FD001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is BAD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def employ_Pipline(     #df: np.array,\n",
    "                        inp_dir: str,\n",
    "                        out_dir: str,\n",
    "                        pipline: Pipeline) -> bool:\n",
    "        \n",
    "        status_log = [\"Preprocess data finished successfull\", \"Preprocess data finished error\"]\n",
    "\n",
    "        # Получаем все документы в папке\n",
    "        inpFilesList = os.listdir(inp_dir)\n",
    "        outFilesList = inpFilesList\n",
    "        status = False\n",
    "        \n",
    "        # Проверяем датасет на пригодность\n",
    "        # if check_type_np(df):\n",
    "        #     print(\"Dataset is GOOD\")\n",
    "        # else:\n",
    "        #     print(\"Dataset is BAD\")\n",
    "        #     status = False\n",
    "        #     return status\n",
    "\n",
    "        # for fl in inpFilesList:\n",
    "        #     print(fl)\n",
    "\n",
    "        # На конце выходной строки дирректории должна стоять \"/\"\n",
    "        if inp_dir[-1] != '/':\n",
    "            inp_dir = f\"{inp_dir}/\"\n",
    "        if out_dir[-1] != '/':\n",
    "            out_dir = f\"{out_dir}/\"\n",
    "\n",
    "        # Применение\n",
    "        try:\n",
    "            for file in inpFilesList:\n",
    "                if file == \".gitkeep\":\n",
    "                    continue\n",
    "                    \n",
    "                print(\"    Processed -> \", file)\n",
    "\n",
    "                dataframe = genfromtxt(inp_dir, delimiter=',')\n",
    "\n",
    "                # Добавляем столбец индексов и удаляем 0-ю строку с названиями столбцов\n",
    "                str_count, col_count = file.shape\n",
    "                indexses = []\n",
    "                indexses.append(0)\n",
    "                \n",
    "                for i in range(str_count-1):\n",
    "                    indexses.append(i)\n",
    "                    print(indexses)\n",
    "\n",
    "                np_Small_BAD_test_FD001 = np.insert(np_Small_BAD_test_FD001, 0, indexses, axis= 1)\n",
    "                np_Small_BAD_test_FD001 = np_Small_BAD_test_FD001[1:, 0:]\n",
    "\n",
    "                dataFrame = (inp_dir + file)\n",
    "                newDataFrame = pipline.fit_transform(dataFrame)\n",
    "                #newDataFrame = pd.DataFrame(dataFrame, columns=pipline['scaler'].get_feature_names_out(dataFrame.columns))\n",
    "                \n",
    "                # Сохранение\n",
    "                filename, extension = os.path.splitext(file)\n",
    "                #newDataFrame.to_pickle(f\"{out_dir}new_{filename}.pickle\")\n",
    "                newDataFrame.save(f\"{out_dir}new_{filename}\", arr = newDataFrame, allow_pickle = True)\n",
    "            \n",
    "            print(status_log[0])\n",
    "            status = True\n",
    "            return status\n",
    "        \n",
    "        except:\n",
    "            print(status_log[1])\n",
    "            status = False\n",
    "            return status\n",
    "    \n",
    "\n",
    "\n",
    "employ_Pipline(#df =np_Small_GOOD_test_FD001,\n",
    "                inp_dir = General_path + raw + tests,\n",
    "                out_dir = General_path + processed,\n",
    "                pipline = pipe_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Источники\n",
    "1 - https://habr.com/ru/companies/yandex_praktikum/articles/756474/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
