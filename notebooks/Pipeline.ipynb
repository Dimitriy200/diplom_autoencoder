{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модуль предобработки данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn==1.1\n",
    "#%pip install --upgrade scikit-learn==1.4.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Имитируем подачу данных с датчиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "General_path = \"C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/\"\n",
    "raw = \"raw/\"\n",
    "processed = \"processed/\"\n",
    "final = \"final/\"\n",
    "tests = \"tests/\"\n",
    "\n",
    "# train_FD001 = pd.read_csv(General_path + \"train_FD001.csv\")\n",
    "# train_FD002 = pd.read_csv(General_path + \"train_FD002.csv\")\n",
    "# train_FD003 = pd.read_csv(General_path + \"train_FD003.csv\")\n",
    "# train_FD004 = pd.read_csv(General_path + \"train_FD004.csv\")\n",
    "\n",
    "# test_FD001 = pd.read_csv(General_path + \"test_FD001.csv\")\n",
    "# test_FD002 = pd.read_csv(General_path + \"test_FD002.csv\")\n",
    "# test_FD003 = pd.read_csv(General_path + \"test_FD003.csv\")\n",
    "# test_FD004 = pd.read_csv(General_path + \"test_FD004.csv\")\n",
    "\n",
    "# This dataset using for show preprocess handling incoming data\n",
    "# BAD_test_FD001 = pd.read_csv(General_path + \"BAD_test_FD001.csv\")\n",
    "\n",
    "# Small_BAD_test_FD001 = pd.read_csv(General_path + raw + \"Small_BAD_test_FD001.csv\")\n",
    "np_Small_BAD_test_FD001 = genfromtxt(General_path + raw + \"Small_BAD_test_FD001.csv\", delimiter=',')\n",
    "np_Small_GOOD_test_FD001 = genfromtxt(General_path + raw + \"Small_GOOD_test_FD001.csv\", delimiter=',')\n",
    "# Small_GOOD_test_FD001 = pd.read_csv(General_path + \"Small_GOOD_test_FD001.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Проверим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_Small_BAD_test_FD001:\n",
      "[[     nan      nan      nan      nan]\n",
      " [ 1.0e+00  1.0e+00      nan  3.0e-04]\n",
      " [ 1.0e+00  2.0e+00 -2.7e-03 -3.0e-04]\n",
      " [ 1.0e+00  3.0e+00  3.0e-04      nan]\n",
      " [ 1.0e+00      nan  4.2e-03  0.0e+00]]\n",
      "\n",
      "np_Small_GOOD_test_FD001:\n",
      "[[     nan      nan      nan      nan]\n",
      " [ 1.0e+00  1.0e+00  2.3e-03  3.0e-04]\n",
      " [ 1.0e+00  2.0e+00 -2.7e-03 -3.0e-04]\n",
      " [ 1.0e+00  3.0e+00  3.0e-04  1.0e-04]\n",
      " [ 1.0e+00  4.0e+00  4.2e-03  0.0e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"np_Small_BAD_test_FD001:\")\n",
    "print(np_Small_BAD_test_FD001)\n",
    "print(\"\\nnp_Small_GOOD_test_FD001:\")\n",
    "print(np_Small_GOOD_test_FD001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск пропусков в pd.Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_null(dataframe: pd.DataFrame):\n",
    "#     count_nulls = 0\n",
    "#     null_set = pd.Dadataframe(dataframe.isnull())\n",
    "\n",
    "#     for col in dataframe.columns:\n",
    "#         for val in dataframe[col].is_null():\n",
    "#             if (val == True):\n",
    "#                 count_nulls +=1\n",
    "#             #print(val)\n",
    "#         print(\"in \", col, \"nulls: \", count_nulls)\n",
    "#         count_nulls = 0\n",
    "\n",
    "# is_null(np_Small_BAD_test_FD001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание и сохранение Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создаем SimpleInputer - простой заполнитель пропущенных значений в датасете\n",
    "\n",
    "Так как предполагается, что данные упорядоченны по циклам работы двигателя, не целесообразно использовать стандартный заполнитель SimpleImputer. разумнее будет вычислить среднее значение лишь до и после пропущенного значения в каждом столбце. Можно использовать KNNImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_inputer = KNNImputer(n_neighbors = 2)\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "pipe_num = Pipeline([('imputer', simple_inputer),('scaler', std_scaler)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучаем Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        , -0.01370506,  0.15075567],\n",
       "       [ 0.        , -1.58113883,  0.05482024,  1.6583124 ],\n",
       "       [ 0.        ,  0.        , -1.52126178, -1.35680105],\n",
       "       [ 0.        ,  1.58113883, -0.15075567, -0.60302269],\n",
       "       [ 0.        ,  0.        ,  1.63090227,  0.15075567]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_num = pipe_num.fit_transform(np_Small_BAD_test_FD001)\n",
    "res_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можем применить полученный pipeline\n",
    "Видим, что пропусков нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_num = pd.DataFrame(res_num, columns=pipe_num['scaler'].get_feature_names_out(np_Small_BAD_test_FD001[0,:]))\n",
    "# res_num\n",
    "\n",
    "res = pipe_num.transform(np_Small_BAD_test_FD001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохраняем Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = 'C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/final/Pipelinr.pickle'\n",
    "\n",
    "with open(savePath, 'wb') as handle:\n",
    "    save_pik_pipeline = pickle.dumps(pipe_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_1.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_dir = 'C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/processed'\n",
    "save_dir = f\"{inp_dir}/\"\n",
    "files = os.listdir(inp_dir)\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    new_name = f'{save_dir}new_{file}'\n",
    "    os.rename(save_dir + file, new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохраняем pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dir = 'C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/raw/tests'\n",
    "out_dir = 'C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/processed'\n",
    "\n",
    "employ_Pipline(inp_dir,\n",
    "               out_dir,\n",
    "               pipe_num) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование data Frame\n",
    "\n",
    "1. Добавляем столбец с индексами\n",
    "2. Удаляем строку с названием столбцов\n",
    "3. Удаляем строку, если в ней есть пропуск "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Добавление столбца с индексами\n",
    "def add_col_indexes(dataFrame: np.array) -> np.array:\n",
    "    \n",
    "    str_count, col_count = dataFrame.shape\n",
    "    indexses = []\n",
    "    indexses.append(0)\n",
    "\n",
    "    print(\"str -->\", str_count,\n",
    "          \"\\ncol -->\", col_count)\n",
    "    \n",
    "    for i in range(str_count-1):\n",
    "        indexses.append(i)\n",
    "\n",
    "    dataFrame = np.insert(dataFrame, 0, indexses, axis= 1)\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Удаление 0-й строки с названиями столбцов\n",
    "def delete_col_names(dataFrame: np.array) -> np.array:\n",
    "    dataFrame = dataFrame[1:, 0:]\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan nan]]\n",
      "[[1.e+00 1.e+00    nan 3.e-04]]\n",
      "[[ 1.0e+00  2.0e+00 -2.7e-03 -3.0e-04]]\n",
      "[[1.e+00 3.e+00 3.e-04    nan]]\n",
      "[[1.        nan 0.0042 0.    ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.0e+00,  2.0e+00, -2.7e-03, -3.0e-04]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delete_nan_str(dataFrame: np.array) -> np.array:\n",
    "    \n",
    "    line, col  = dataFrame.shape\n",
    "    res_dataFrame = np.empty((1, col))\n",
    "\n",
    "    for line in dataFrame:\n",
    "\n",
    "        line = np.array([line])\n",
    "        print(line)\n",
    "\n",
    "        if(check_nan_dataFrame(line) == False):\n",
    "            res_dataFrame = np.vstack((res_dataFrame, line))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    res_dataFrame = delete_col_names(res_dataFrame)\n",
    "    \n",
    "    return res_dataFrame\n",
    "\n",
    "delete_nan_str(np_Small_BAD_test_FD001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str --> 5 \n",
      "col --> 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.0e+00,  1.0e+00,  1.0e+00,  2.3e-03,  3.0e-04],\n",
       "       [ 1.0e+00,  1.0e+00,  2.0e+00, -2.7e-03, -3.0e-04],\n",
       "       [ 2.0e+00,  1.0e+00,  3.0e+00,  3.0e-04,  1.0e-04],\n",
       "       [ 3.0e+00,  1.0e+00,  4.0e+00,  4.2e-03,  0.0e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_standardization_df(dataFrame: np.array) -> np.array:\n",
    "    \n",
    "    dataFrame = add_col_indexes(dataFrame)\n",
    "    dataFrame = delete_col_names(dataFrame)\n",
    "    dataFrame = delete_nan_str(dataFrame)\n",
    "\n",
    "    return dataFrame\n",
    "\n",
    "to_standardization_df(np_Small_BAD_test_FD001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Из NumPy в Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np_Small_BAD_test_FD001[1:,1:].astype(float)\n",
    "colums = np_Small_BAD_test_FD001[0, 1:]\n",
    "\n",
    "pd_Small_BAD_test_FD001 = pd.DataFrame(values, columns = colums)\n",
    "pd_Small_BAD_test_FD001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ф-ии проверки данных / наличия nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_nan_dataFrame(dataset: np.array) -> bool: #\n",
    "\n",
    "   #dataset_v = np.delete(dataset, (0), axis=0)\n",
    "    res: bool\n",
    "    res = False\n",
    "\n",
    "    for st in dataset:\n",
    "        for col in st:\n",
    "            # print(f\"{col} --> {type(col)}\")\n",
    "            if(np.isnan(col)):\n",
    "                res = True\n",
    "                return res\n",
    "    \n",
    "    return res\n",
    "            \n",
    "\n",
    "check_nan_dataFrame(np_Small_GOOD_test_FD001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции для модуля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is BAD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def employ_Pipline(     #df: np.array,\n",
    "                        inp_dir: str,\n",
    "                        out_dir: str,\n",
    "                        pipline: Pipeline) -> bool:\n",
    "        \n",
    "        status_log = [\"Preprocess data finished successfull\", \"Preprocess data finished error\"]\n",
    "\n",
    "        # Получаем все документы в папке\n",
    "        inpFilesList = os.listdir(inp_dir)\n",
    "        outFilesList = inpFilesList\n",
    "        status = False\n",
    "        \n",
    "        # Проверяем датасет на пригодность\n",
    "        # if check_type_np(df):\n",
    "        #     print(\"Dataset is GOOD\")\n",
    "        # else:\n",
    "        #     print(\"Dataset is BAD\")\n",
    "        #     status = False\n",
    "        #     return status\n",
    "\n",
    "        # for fl in inpFilesList:\n",
    "        #     print(fl)\n",
    "\n",
    "        # На конце выходной строки дирректории должна стоять \"/\"\n",
    "        if inp_dir[-1] != '/':\n",
    "            inp_dir = f\"{inp_dir}/\"\n",
    "        if out_dir[-1] != '/':\n",
    "            out_dir = f\"{out_dir}/\"\n",
    "\n",
    "        # Применение\n",
    "        try:\n",
    "            for file in inpFilesList:\n",
    "                if file == \".gitkeep\":\n",
    "                    continue\n",
    "                    \n",
    "                print(\"    Processed -> \", file)\n",
    "\n",
    "                dataframe = genfromtxt(inp_dir, delimiter=',')\n",
    "\n",
    "                # Добавляем столбец индексов и удаляем 0-ю строку с названиями столбцов\n",
    "                str_count, col_count = file.shape\n",
    "                indexses = []\n",
    "                indexses.append(0)\n",
    "                \n",
    "                for i in range(str_count-1):\n",
    "                    indexses.append(i)\n",
    "                    print(indexses)\n",
    "\n",
    "                np_Small_BAD_test_FD001 = np.insert(np_Small_BAD_test_FD001, 0, indexses, axis= 1)\n",
    "                np_Small_BAD_test_FD001 = np_Small_BAD_test_FD001[1:, 0:]\n",
    "\n",
    "                dataFrame = (inp_dir + file)\n",
    "                newDataFrame = pipline.fit_transform(dataFrame)\n",
    "                #newDataFrame = pd.DataFrame(dataFrame, columns=pipline['scaler'].get_feature_names_out(dataFrame.columns))\n",
    "                \n",
    "                # Сохранение\n",
    "                filename, extension = os.path.splitext(file)\n",
    "                #newDataFrame.to_pickle(f\"{out_dir}new_{filename}.pickle\")\n",
    "                newDataFrame.save(f\"{out_dir}new_{filename}\", arr = newDataFrame, allow_pickle = True)\n",
    "            \n",
    "            print(status_log[0])\n",
    "            status = True\n",
    "            return status\n",
    "        \n",
    "        except:\n",
    "            print(status_log[1])\n",
    "            status = False\n",
    "            return status\n",
    "    \n",
    "\n",
    "\n",
    "employ_Pipline(#df =np_Small_GOOD_test_FD001,\n",
    "                inp_dir = General_path + raw + tests,\n",
    "                out_dir = General_path + processed,\n",
    "                pipline = pipe_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Источники\n",
    "1 - https://habr.com/ru/companies/yandex_praktikum/articles/756474/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
