{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модуль предобработки данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn==1.1\n",
    "#%pip install --upgrade scikit-learn==1.4.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from numpy import genfromtxt\n",
    "from tempfile import TemporaryFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Имитируем подачу данных с датчиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/processed/New_train_FD001.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m\n\u001b[0;32m      6\u001b[0m jsons \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjsonss/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# train_FD001 = pd.read_csv(General_path + \"train_FD001.csv\")\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# train_FD002 = pd.read_csv(General_path + \"train_FD002.csv\")\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# train_FD003 = pd.read_csv(General_path + \"train_FD003.csv\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# np_Small_BAD_test_FD001 = genfromtxt(General_path + raw + \"Small_BAD_test_FD001.csv\", delimiter=',')\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# np_Small_GOOD_test_FD001 = genfromtxt(General_path + raw + \"Small_GOOD_test_FD001.csv\", delimiter=',')\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m New_train_FD001 \u001b[38;5;241m=\u001b[39m \u001b[43mgenfromtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGeneral_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNew_train_FD001.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m New_train_FD002 \u001b[38;5;241m=\u001b[39m genfromtxt(General_path \u001b[38;5;241m+\u001b[39m processed \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew_train_FD002.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m New_train_FD003 \u001b[38;5;241m=\u001b[39m genfromtxt(General_path \u001b[38;5;241m+\u001b[39m processed \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew_train_FD003.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dmitriy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py:1980\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[0;32m   1978\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os_fspath(fname)\n\u001b[0;32m   1979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1980\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1981\u001b[0m     fid_ctx \u001b[38;5;241m=\u001b[39m contextlib\u001b[38;5;241m.\u001b[39mclosing(fid)\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Dmitriy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dmitriy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/processed/New_train_FD001.csv not found."
     ]
    }
   ],
   "source": [
    "General_path = \"C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/\"\n",
    "raw = \"raw/\"\n",
    "processed = \"processed/\"\n",
    "final = \"final/\"\n",
    "tests = \"tests/\"\n",
    "jsons = \"jsonss/\"\n",
    "\n",
    "# train_FD001 = pd.read_csv(General_path + \"train_FD001.csv\")\n",
    "# train_FD002 = pd.read_csv(General_path + \"train_FD002.csv\")\n",
    "# train_FD003 = pd.read_csv(General_path + \"train_FD003.csv\")\n",
    "# train_FD004 = pd.read_csv(General_path + \"train_FD004.csv\")\n",
    "\n",
    "# test_FD001 = pd.read_csv(General_path + \"test_FD001.csv\")\n",
    "# test_FD002 = pd.read_csv(General_path + \"test_FD002.csv\")\n",
    "# test_FD003 = pd.read_csv(General_path + \"test_FD003.csv\")\n",
    "# test_FD004 = pd.read_csv(General_path + \"test_FD004.csv\")\n",
    "\n",
    "# This dataset using for show preprocess handling incoming data\n",
    "# BAD_test_FD001 = pd.read_csv(General_path + \"BAD_test_FD001.csv\")\n",
    "\n",
    "# Small_BAD_test_FD001 = pd.read_csv(General_path + raw + \"Small_BAD_test_FD001.csv\")\n",
    "# np_Small_BAD_test_FD001 = genfromtxt(General_path + raw + \"Small_BAD_test_FD001.csv\", delimiter=',')\n",
    "# np_Small_GOOD_test_FD001 = genfromtxt(General_path + raw + \"Small_GOOD_test_FD001.csv\", delimiter=',')\n",
    "\n",
    "# New_train_FD001 = genfromtxt(General_path + processed + \"New_train_FD001.csv\", delimiter=',')\n",
    "# New_train_FD002 = genfromtxt(General_path + processed + \"New_train_FD002.csv\", delimiter=',')\n",
    "# New_train_FD003 = genfromtxt(General_path + processed + \"New_train_FD003.csv\", delimiter=',')\n",
    "# New_train_FD004 = genfromtxt(General_path + processed + \"New_train_FD004.csv\", delimiter=',')\n",
    "\n",
    "# Small_GOOD_test_FD001 = pd.read_csv(General_path + \"Small_GOOD_test_FD001.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small_BAD_test_FD001.csv\n"
     ]
    }
   ],
   "source": [
    "os.path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Проверим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_Small_BAD_test_FD001:\n",
      "[[-1.72812544 -1.56520816 -0.31598734 ...  0.          1.34852542\n",
      "   1.19445599]\n",
      " [-1.72812544 -1.55068966  0.87274308 ...  0.          1.01655257\n",
      "   1.23695194]\n",
      " [-1.72812544 -1.53617116 -1.96192176 ...  0.          0.73990852\n",
      "   0.50343501]\n",
      " ...\n",
      " [ 1.65924428  1.29493582  0.18693707 ...  0.         -2.08186078\n",
      "  -3.29256126]\n",
      " [ 1.65924428  1.30945432 -0.49886894 ...  0.         -2.91179293\n",
      "  -2.08512219]\n",
      " [ 1.65924428  1.32397281 -1.45899735 ...  0.         -2.46916245\n",
      "  -2.19413352]]\n"
     ]
    }
   ],
   "source": [
    "print(\"np_Small_BAD_test_FD001:\")\n",
    "print(New_train_FD001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOD_dataFrame = np.array([[1, 1, 1],\n",
    "                           [1, 1, 1],\n",
    "                           [1, 1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск пропусков в pd.Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_null(dataframe: pd.DataFrame):\n",
    "#     count_nulls = 0\n",
    "#     null_set = pd.Dadataframe(dataframe.isnull())\n",
    "\n",
    "#     for col in dataframe.columns:\n",
    "#         for val in dataframe[col].is_null():\n",
    "#             if (val == True):\n",
    "#                 count_nulls +=1\n",
    "#             #print(val)\n",
    "#         print(\"in \", col, \"nulls: \", count_nulls)\n",
    "#         count_nulls = 0\n",
    "\n",
    "# is_null(np_Small_BAD_test_FD001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание и сохранение Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создаем SimpleInputer - простой заполнитель пропущенных значений в датасете\n",
    "\n",
    "Так как предполагается, что данные упорядоченны по циклам работы двигателя, не целесообразно использовать стандартный заполнитель SimpleImputer. разумнее будет вычислить среднее значение лишь до и после пропущенного значения в каждом столбце. Можно использовать KNNImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_inputer = KNNImputer(n_neighbors = 2)\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "pipe_num = Pipeline([('imputer', simple_inputer),('scaler', std_scaler)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучаем Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        , -0.01370506,  0.15075567],\n",
       "       [ 0.        , -1.58113883,  0.05482024,  1.6583124 ],\n",
       "       [ 0.        ,  0.        , -1.52126178, -1.35680105],\n",
       "       [ 0.        ,  1.58113883, -0.15075567, -0.60302269],\n",
       "       [ 0.        ,  0.        ,  1.63090227,  0.15075567]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_num = pipe_num.fit_transform(np_Small_BAD_test_FD001)\n",
    "res_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можем применить полученный pipeline\n",
    "Видим, что пропусков нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_num = pd.DataFrame(res_num, columns=pipe_num['scaler'].get_feature_names_out(np_Small_BAD_test_FD001[0,:]))\n",
    "# res_num\n",
    "\n",
    "res = pipe_num.transform(np_Small_BAD_test_FD001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохраняем Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = 'C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/final/Pipelinr.pickle'\n",
    "\n",
    "with open(savePath, 'wb') as handle:\n",
    "    save_pik_pipeline = pickle.dumps(pipe_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_1.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_dir = 'C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/processed'\n",
    "save_dir = f\"{inp_dir}/\"\n",
    "files = os.listdir(inp_dir)\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    new_name = f'{save_dir}new_{file}'\n",
    "    os.rename(save_dir + file, new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохраняем pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dir = 'C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/raw/tests'\n",
    "out_dir = 'C:/Users/Dmitriy/Desktop/Univer/Diplom/diplom_autoencoder/data/processed'\n",
    "\n",
    "employ_Pipline(inp_dir,\n",
    "               out_dir,\n",
    "               pipe_num) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование data Frame\n",
    "\n",
    "1. Добавляем столбец с индексами\n",
    "2. Удаляем строку с названием столбцов\n",
    "3. Удаляем строку, если в ней есть пропуск "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Добавление столбца с индексами\n",
    "def add_col_indexes(dataFrame: np.array) -> np.array:\n",
    "    \n",
    "    str_count, col_count = dataFrame.shape\n",
    "    indexses = []\n",
    "    indexses.append(0)\n",
    "\n",
    "    print(\"str -->\", str_count,\n",
    "          \"\\ncol -->\", col_count)\n",
    "    \n",
    "    for i in range(str_count-1):\n",
    "        indexses.append(i)\n",
    "\n",
    "    dataFrame = np.insert(dataFrame, 0, indexses, axis= 1)\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Удаление 0-й строки с названиями столбцов\n",
    "def delete_col_names(dataFrame: np.array) -> np.array:\n",
    "    dataFrame = dataFrame[1:, 0:]\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan nan]]\n",
      "[[1.e+00 1.e+00    nan 3.e-04]]\n",
      "[[ 1.0e+00  2.0e+00 -2.7e-03 -3.0e-04]]\n",
      "[[1.e+00 3.e+00 3.e-04    nan]]\n",
      "[[1.        nan 0.0042 0.    ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.0e+00,  2.0e+00, -2.7e-03, -3.0e-04]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delete_nan_str(dataFrame: np.array) -> np.array:\n",
    "    \n",
    "    line, col  = dataFrame.shape\n",
    "    res_dataFrame = np.empty((1, col))\n",
    "\n",
    "    for line in dataFrame:\n",
    "\n",
    "        line = np.array([line])\n",
    "        print(line)\n",
    "\n",
    "        if(check_nan_dataFrame(line) == False):\n",
    "            res_dataFrame = np.vstack((res_dataFrame, line))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    res_dataFrame = delete_col_names(res_dataFrame)\n",
    "    \n",
    "    return res_dataFrame\n",
    "\n",
    "delete_nan_str(np_Small_BAD_test_FD001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str --> 5 \n",
      "col --> 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.0e+00,  1.0e+00,  1.0e+00,  2.3e-03,  3.0e-04],\n",
       "       [ 1.0e+00,  1.0e+00,  2.0e+00, -2.7e-03, -3.0e-04],\n",
       "       [ 2.0e+00,  1.0e+00,  3.0e+00,  3.0e-04,  1.0e-04],\n",
       "       [ 3.0e+00,  1.0e+00,  4.0e+00,  4.2e-03,  0.0e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_standardization_df(dataFrame: np.array) -> np.array:\n",
    "    \n",
    "    dataFrame = add_col_indexes(dataFrame)\n",
    "    dataFrame = delete_col_names(dataFrame)\n",
    "    dataFrame = delete_nan_str(dataFrame)\n",
    "\n",
    "    return dataFrame\n",
    "\n",
    "to_standardization_df(np_Small_BAD_test_FD001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Из NumPy в Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np_Small_BAD_test_FD001[1:,1:].astype(float)\n",
    "colums = np_Small_BAD_test_FD001[0, 1:]\n",
    "\n",
    "pd_Small_BAD_test_FD001 = pd.DataFrame(values, columns = colums)\n",
    "pd_Small_BAD_test_FD001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ф-ии проверки данных / наличия nan\n",
    "Для указания строки используй --> df[str, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_info(status: bool, text: str):\n",
    "    print(f\"{text} result is {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.0e+00  2.0e+00 -2.7e-03 -3.0e-04]\n",
      "Check Nan Line dataframe error result is False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_nan_dataFrame_Line(dataset: np.array) -> bool: #\n",
    "\n",
    "   #dataset_v = np.delete(dataset, (0), axis=0)\n",
    "    status_log = [\"Check Nan Line dataframe successfull\", \"Check Nan Line dataframe error\"]\n",
    "    res: bool = False\n",
    "\n",
    "    for st in dataset:\n",
    "        # print(f\"{col} --> {type(col)}\")\n",
    "        if(np.isnan(st)):\n",
    "            res = True\n",
    "            out_info(res, status_log[0])\n",
    "            return res\n",
    "        \n",
    "    out_info(res, status_log[1])\n",
    "    return res\n",
    "\n",
    "print(np_Small_BAD_test_FD001[2, :])\n",
    "is_nan_dataFrame_Line(np_Small_BAD_test_FD001[2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_named_col(dataset: np.array) -> bool:\n",
    "\n",
    "        status_log = [\"Check first string dataframe successfull\", \"Check first string dataframe error\"]\n",
    "        res = False\n",
    "\n",
    "        first_str = dataset[0, :]\n",
    "        print(first_str)\n",
    "\n",
    "        for val in first_str:\n",
    "            if(np.isnan(val)):\n",
    "                res = True\n",
    "            else:\n",
    "                res = False\n",
    "                return res\n",
    "        \n",
    "        return res\n",
    "\n",
    "\n",
    "is_named_col(np_Small_BAD_test_FD001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str = 5\n",
      "col = 4\n",
      "Check Nan Line dataframe successfull result is True\n",
      "Check Nan Line dataframe successfull result is True\n",
      "Check Nan Line dataframe error result is False\n",
      "res_dataFrame = [[ 0.0e+00  0.0e+00  0.0e+00  0.0e+00]\n",
      " [ 1.0e+00  2.0e+00 -2.7e-03 -3.0e-04]]\n",
      "Check Nan Line dataframe successfull result is True\n",
      "Check Nan Line dataframe successfull result is True\n",
      "Delete NULL lines in dataframe successfull result is True\n",
      "res =  [[ 1.0e+00  2.0e+00 -2.7e-03 -3.0e-04]]\n"
     ]
    }
   ],
   "source": [
    "def delete_nan_str(dataFrame: np.array) -> np.array:\n",
    "    \n",
    "        status_log = [\"Delete NULL lines in dataframe successfull\", \"Delete NULL lines in dataframe error\"]\n",
    "        status = False\n",
    "\n",
    "        str, col  = dataFrame.shape\n",
    "        print(f\"str = {str}\\ncol = {col}\")\n",
    "        res_dataFrame = np.zeros(col)\n",
    "\n",
    "        for line in dataFrame:\n",
    "            if(is_nan_dataFrame_Line(line) == False):\n",
    "                res_dataFrame = np.vstack((res_dataFrame, line))\n",
    "                print(f\"res_dataFrame = {res_dataFrame}\")\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        res_dataFrame = res_dataFrame[1:, :]\n",
    "        \n",
    "        out_info(True, status_log[0])\n",
    "        return res_dataFrame\n",
    "\n",
    "\n",
    "res = delete_nan_str(np_Small_BAD_test_FD001)\n",
    "print(\"res = \", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n",
      "str --> 3 \n",
      "col --> 3\n",
      "Check Nan Line dataframe error result is False\n",
      "Add column with indexes in dataframe successfull result is True\n",
      "res = \n",
      " [[0 1 1 1]\n",
      " [1 1 1 1]\n",
      " [2 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "def add_col_indexes(dataFrame: np.array) -> np.array:\n",
    "        \n",
    "        status_log = [\"Add column with indexes in dataframe successfull\", \"Add column with indexes in dataframe error\"]\n",
    "\n",
    "        str_count, col_count = dataFrame.shape\n",
    "        indexses = []\n",
    "        coef_if_empty = 0\n",
    "\n",
    "        print(\"str -->\", str_count,\n",
    "            \"\\ncol -->\", col_count)\n",
    "\n",
    "        if (is_nan_dataFrame_Line(dataFrame[0, :])):\n",
    "            indexses.append(np.nan)\n",
    "            coef_if_empty = 1\n",
    "        \n",
    "        for i in range(str_count - coef_if_empty):\n",
    "            indexses.append(i)\n",
    "\n",
    "        dataFrame = np.insert(dataFrame, 0, indexses, axis= 1)\n",
    "\n",
    "        out_info(True, status_log[0])\n",
    "        return dataFrame\n",
    "\n",
    "\n",
    "print(GOOD_dataFrame)\n",
    "print(\"res = \\n\", add_col_indexes(GOOD_dataFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_names(dataFrame: np.array) -> np.array:\n",
    "    \n",
    "    status_log = [\"Delete columns with names in dataframe successfull\", \"Delete columns with names in dataframe error\"]\n",
    "\n",
    "    if(is_named_col(dataFrame) == True):\n",
    "        dataFrame = dataFrame[1:, 0:]\n",
    "\n",
    "    out_info(True, status_log[0])\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_standardization_df(dataFrame: np.array) -> np.array:\n",
    "\n",
    "    status_log = [\"Standartization dataframe successfull\", \"Standartization dataframe error\"]\n",
    "\n",
    "    dataFrame = add_col_indexes(dataFrame)\n",
    "    dataFrame = delete_names(dataFrame)\n",
    "    dataFrame = delete_nan_str(dataFrame)\n",
    "\n",
    "    out_info(True, status_log[0])\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции для модуля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting a list of documents...\n",
      "   .../Small_BAD_test_FD001.csv\n",
      "   .../Small_GOOD_test_FD001.csv\n",
      "Documents have been received\n",
      "  Processed -->  Small_BAD_test_FD001.csv\n",
      "Check Nan Line dataframe successfull result is True\n",
      "Dataset is BAD Starting standartization dataframe...\n",
      "str --> 5 \n",
      "col --> 4\n",
      "Check Nan Line dataframe successfull result is True\n",
      "Add column with indexes in dataframe successfull result is True\n",
      "[nan nan nan nan nan]\n",
      "Delete columns with names in dataframe successfull result is True\n",
      "str = 4\n",
      "col = 5\n",
      "Check Nan Line dataframe successfull result is True\n",
      "Check Nan Line dataframe error result is False\n",
      "res_dataFrame = [[ 0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00]\n",
      " [ 1.0e+00  1.0e+00  2.0e+00 -2.7e-03 -3.0e-04]]\n",
      "Check Nan Line dataframe successfull result is True\n",
      "Check Nan Line dataframe successfull result is True\n",
      "Delete NULL lines in dataframe successfull result is True\n",
      "Standartization dataframe successfull result is True\n",
      "Dataset is GOOD  Starting employ pipeline...\n",
      "  Processed -->  Small_GOOD_test_FD001.csv\n",
      "Check Nan Line dataframe error result is False\n",
      "Check Nan Line dataframe error result is False\n",
      "Check Nan Line dataframe error result is False\n",
      "Check Nan Line dataframe error result is False\n",
      "Dataset is GOOD  Starting employ pipeline...\n",
      "Preprocess data finished successfull result is True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def employ_Pipline(inp_dir: str,       # B inpFilesList и outFilesList указывать полный путь\n",
    "                    out_dir: str,\n",
    "                    pipline: Pipeline = pipe_num) -> bool:\n",
    "    \n",
    "    status_log  =           [\"Preprocess data finished successfull\",        \"Preprocess data finished error\"]\n",
    "    get_doc_log =           [\"Getting a list of documents...\",              \"Documents have been received\"]\n",
    "\n",
    "    # На конце выходной строки дирректории должна стоять \"/\"\n",
    "    if inp_dir[-1] != '/':\n",
    "        inp_dir = f\"{inp_dir}/\"\n",
    "    if out_dir[-1] != '/':\n",
    "        out_dir = f\"{out_dir}/\"\n",
    "\n",
    "    # Получаем все документы в папке\n",
    "    print(get_doc_log[0])\n",
    "    inpFilesList = os.listdir(inp_dir)\n",
    "    outFilesList = inpFilesList\n",
    "    \n",
    "    # Вывод списка файлов в диррекории\n",
    "    for fl in inpFilesList:\n",
    "        print(\"   .../\" + fl)\n",
    "        \n",
    "    print(get_doc_log[1])\n",
    "\n",
    "    # Применение\n",
    "    for file in inpFilesList:\n",
    "        if file == \".gitkeep\":\n",
    "            continue\n",
    "        \n",
    "        print(\"  Processed --> \", file)\n",
    "\n",
    "        # Загружаем i-тый фаил\n",
    "        dataFrame = genfromtxt(inp_dir + file, delimiter = ',')\n",
    "        \n",
    "        # Проверяем датасет на пригодность (отсутствие пропусков)\n",
    "        for line in dataFrame:\n",
    "            if (is_nan_dataFrame_Line(line) == False):\n",
    "                continue\n",
    "            else:\n",
    "                print(\"Dataset is BAD Starting standartization dataframe...\")\n",
    "                to_standardization_df(dataFrame)\n",
    "                break\n",
    "        \n",
    "        print(\"Dataset is GOOD  Starting employ pipeline...\")\n",
    "        \n",
    "        # newDataFrame = pd.DataFrame(dataFrame, columns = pipline['scaler'].get_feature_names_out(dataFrame.columns))\n",
    "        newDataFrame = pipline.fit_transform(dataFrame)\n",
    "\n",
    "        # Сохранение\n",
    "        fileName = out_dir + \"New_\" + file + \".csv\"\n",
    "        np.savetxt(fileName, newDataFrame, delimiter = \",\")\n",
    "    \n",
    "    out_info(True, status_log[0])\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inp_dir = General_path + raw + tests\n",
    "out_dir = General_path + processed\n",
    "\n",
    "employ_Pipline(inp_dir,\n",
    "                out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделение датасета на нормальные и аномальные значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiation_on_anomaly(dataFrame: np.array,\n",
    "                               last_anomaly: int,\n",
    "                               out_dir: str,\n",
    "                               file_name: str):\n",
    "    \n",
    "    count_unit_number = dataFrame[1, 0]\n",
    "    count_time_cycles = 1\n",
    "\n",
    "    current_str_num = 0\n",
    "    count = 0\n",
    "\n",
    "    last_time_cycles = []\n",
    "\n",
    "    str_df, col_df  = dataFrame.shape\n",
    "\n",
    "    normal_df = np.zeros(col_df)\n",
    "    anomalmal_df = np.zeros(col_df)\n",
    "\n",
    "    unit_number = 0\n",
    "\n",
    "    fileName_Normal_DF = out_dir + \"New_NORMAL\" + file_name + \".csv\"\n",
    "    fileName_Anomal_DF = out_dir + \"New_ANOMAL\" + file_name + \".csv\"\n",
    "    \n",
    "\n",
    "    for str in dataFrame:\n",
    "        unit_number = dataFrame[current_str_num, 0]\n",
    "        #print(unit_number)\n",
    "\n",
    "        if(count_unit_number != unit_number or\n",
    "           current_str_num == str_df - 1):\n",
    "            last_time_cycles.append(count_time_cycles)\n",
    "            count_unit_number = unit_number\n",
    "        \n",
    "        count_time_cycles += 1\n",
    "        current_str_num += 1\n",
    "\n",
    "    count_unit_number = dataFrame[1, 0]\n",
    "    count_time_cycles = 1\n",
    "    current_str_num = 0\n",
    "\n",
    "    for str in dataFrame:\n",
    "\n",
    "        unit_number = dataFrame[current_str_num, 0]\n",
    "\n",
    "        if(count_unit_number != unit_number):\n",
    "            count += 1\n",
    "            count_unit_number = unit_number\n",
    "\n",
    "        barrer = last_time_cycles[count] - last_anomaly\n",
    "\n",
    "        if(count_time_cycles < barrer):\n",
    "            normal_df = np.vstack((normal_df, str))\n",
    "        \n",
    "        else: \n",
    "            anomalmal_df = np.vstack((anomalmal_df, str))\n",
    "\n",
    "        count_time_cycles += 1\n",
    "        current_str_num += 1\n",
    "\n",
    "    # normal_df = normal_df[1:, :]\n",
    "    # anomalmal_df = anomalmal_df[1:, :]\n",
    "\n",
    "\n",
    "    np.savetxt(fileName_Normal_DF, normal_df, delimiter = \",\")\n",
    "    np.savetxt(fileName_Anomal_DF, anomalmal_df, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = General_path + processed\n",
    "last_anomaly = 50\n",
    "file_name = \"New_train_FD001\"\n",
    "\n",
    "differentiation_on_anomaly(New_train_FD001,\n",
    "                           last_anomaly,\n",
    "                           out_dir,\n",
    "                           file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time_in_cycles_1.json', 'time_in_cycles_2.json', 'time_in_cycles_3.json', 'time_in_cycles_4.json', 'time_in_cycles_5.json']\n"
     ]
    }
   ],
   "source": [
    "def load_json(inp_json_dir: str,\n",
    "              out_csv_dir: str,\n",
    "              name_out_csv: str):\n",
    "\n",
    "    res_arr = []\n",
    "\n",
    "    jsons = os.listdir(inp_json_dir)\n",
    "    print(jsons)\n",
    "\n",
    "    for file in jsons:\n",
    "        json_dict = {}\n",
    "        with open(inp_json_dir + file, \"r\") as json_file:\n",
    "            json_dict = json.load(json_file)\n",
    "            time_res = list(json_dict.values())\n",
    "            res_arr.append(time_res)\n",
    "    \n",
    "    with open(os.path.join(out_csv_dir, name_out_csv), \"w\") as new_csv:\n",
    "        write = csv.writer(new_csv, lineterminator='\\n')\n",
    "        write.writerows(res_arr)\n",
    "\n",
    "\n",
    "\n",
    "load_json(os.path.join(General_path, raw, jsons),\n",
    "          os.path.join(General_path, final),\n",
    "          \"New_json_out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Всякие тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [2, 2, 2, 2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [[1, 1, 1, 1],\n",
    "       [2, 2, 2, 2]]\n",
    "\n",
    "res = np.array(arr)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [1, 2, 3],\n",
    "                [1, 2, 3],\n",
    "                [1, 2, 3],\n",
    "                [1, 2, 3],\n",
    "                [1, 2, 3],\n",
    "                [1, 2, 3],\n",
    "                [1, 2, 3],\n",
    "                [1, 2, 3],\n",
    "                [1, 2, 3],\n",
    "                [1, 2, 3],\n",
    "                [1, 2, 3],\n",
    "                [1, 2, 3],\n",
    "                [1, 2, 3],\n",
    "                [2, 11, 12],\n",
    "                [2, 11, 12],\n",
    "                [2, 11, 12],\n",
    "                [2, 11, 12],\n",
    "                [2, 11, 12],\n",
    "                [2, 11, 12],\n",
    "                [2, 11, 12],\n",
    "                [2, 11, 12],\n",
    "                [2, 11, 12],\n",
    "                [2, 11, 12],\n",
    "                [2, 11, 12],\n",
    "                [2, 11, 12],\n",
    "                [2, 11, 12],\n",
    "                [2, 11, 12],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],\n",
    "                [3, 14, 15],])\n",
    "\n",
    "\n",
    "unit_numbers = arr[:, 0].tolist()\n",
    "\n",
    "# count_repeat = 0\n",
    "# repeat_i = 0\n",
    "# prev_item = 0\n",
    "\n",
    "# for unit in unit_numbers:\n",
    "\n",
    "#     if unit == prev_item:\n",
    "#         repeat_i += 1\n",
    "\n",
    "\n",
    "num_count = 0\n",
    "for num in set(unit_numbers):\n",
    "    if num_count == 0:\n",
    "        num_count = unit_numbers.count(num)\n",
    "    elif unit_numbers.count(num) < num_count:\n",
    "        num_count = unit_numbers.count(num)\n",
    "\n",
    "num_count = num_count / 10\n",
    "num_count = round(num_count, 0)\n",
    "print(f\"{num_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Источники\n",
    "1 - https://habr.com/ru/companies/yandex_praktikum/articles/756474/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
